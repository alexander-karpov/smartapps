{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yargy import ( Parser, rule, or_, and_, not_ )\n",
    "from yargy.predicates import ( true, gram, eq, in_, type, normalized, dictionary, gte, lte, AndPredicate )\n",
    "from yargy.pipelines import ( pipeline, morph_pipeline )\n",
    "from yargy.interpretation import ( fact, attribute )\n",
    "from yargy.relations import gnc_relation, number_relation, main\n",
    "from yargy.predicates.constructors import ParameterPredicateScheme, ParameterPredicate\n",
    "from yargy.predicates.bank import morph_required\n",
    "\n",
    "\n",
    "def AndPredicate_constrain(self, token):\n",
    "    if not hasattr(token, 'constrained'):\n",
    "        return token\n",
    "\n",
    "    forms = None\n",
    "\n",
    "    for p in self.predicates:\n",
    "        if p(token):\n",
    "            constrained = p.constrain(token)\n",
    "\n",
    "            if not hasattr(constrained, 'forms'):\n",
    "                continue\n",
    "\n",
    "            if not forms:\n",
    "                forms = set(p.constrain(token).forms)\n",
    "            else:\n",
    "                forms &= set(p.constrain(token).forms)\n",
    "\n",
    "    return token.constrained(list(forms))\n",
    "\n",
    "AndPredicate.constrain = AndPredicate_constrain\n",
    "\n",
    "\n",
    "class grams(ParameterPredicateScheme):\n",
    "    def activate(self, context):\n",
    "        for gr in self.value:\n",
    "            context.tokenizer.morph.check_gram(gr)\n",
    "        return GramsPredicate(self.value)\n",
    "\n",
    "\n",
    "class GramsPredicate(ParameterPredicate):\n",
    "    @morph_required\n",
    "    def __call__(self, token):\n",
    "        return any(\n",
    "            all((v in _.grams for v in self.value))\n",
    "            for _ in token.forms\n",
    "        )\n",
    "\n",
    "    def constrain(self, token):\n",
    "        return token.constrained([\n",
    "            _ for _ in token.forms\n",
    "            if all((v in _.grams for v in self.value))\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def label(self):\n",
    "        return \"gram('%s')\" % self.value\n",
    "\n",
    "\n",
    "class not_grams(ParameterPredicateScheme):\n",
    "    def activate(self, context):\n",
    "        for gr in self.value:\n",
    "            context.tokenizer.morph.check_gram(gr)\n",
    "        return NotGramsPredicate(self.value)\n",
    "\n",
    "\n",
    "class NotGramsPredicate(ParameterPredicate):\n",
    "    @morph_required\n",
    "    def __call__(self, token):\n",
    "        return any(\n",
    "            True\n",
    "            for _ in token.forms\n",
    "            if not all((v in _.grams for v in self.value))\n",
    "        )\n",
    "\n",
    "    def constrain(self, token):\n",
    "        return token.constrained([\n",
    "            _ for _ in token.forms\n",
    "            if not all((v in _.grams for v in self.value))\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def label(self):\n",
    "        return \"gram('%s')\" % self.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "красный молочай\n",
      "красный молочай\n",
      "красный молочай\n",
      "красный молочай\n",
      "красный молочай\n",
      "None\n",
      "None\n",
      "красный молочай\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "красный молочай\n",
      "красный молочай\n",
      "красный молочай\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "красный молочай\n",
      "красный молочай\n",
      "красный молочай\n",
      "красный молочай\n",
      "красный молочай\n",
      "красный молочай\n",
      "None\n",
      "красный молочай\n",
      "красный молочай\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pymorphy2.readthedocs.io/en/stable/user/grammemes.html\n",
    "# http://opencorpora.org/dict.php?act=gram\n",
    "\n",
    "from db import db\n",
    "from typing import Optional\n",
    "import pymorphy2\n",
    "\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "Collocation = fact(\n",
    "    'Collocation',\n",
    "    [attribute('main').repeatable(), attribute('dependent').repeatable()]\n",
    ")\n",
    "\n",
    "Entity = fact(\n",
    "    'Entity',\n",
    "    ['subject', 'predicate', 'prep']\n",
    ")\n",
    "\n",
    "# NOUN = or_(\n",
    "#     and_(\n",
    "#         gram(\"NOUN\"),\n",
    "#         not_(gram(\"PREP\")), # Предлоги\n",
    "#         not_(gram(\"PRCL\")), # Частицы\n",
    "#         not_(eq('алиса')),\n",
    "#     ),\n",
    "#     # больной проказой\n",
    "#     and_(\n",
    "#         gram(\"ADJF\"),\n",
    "#         gram(\"Subx\"),\n",
    "#     ),\n",
    "#     # радиоведущий\n",
    "#     # телевизионный ведущий\n",
    "#     # заключённый, прокаженный\n",
    "#     and_(\n",
    "#         gram(\"PRTF\"),\n",
    "#         gram(\"Subx\"),\n",
    "#     ),\n",
    "#     eq('хрен'), # Является частицей\n",
    "#     eq('уж'), # Является частицей\n",
    "#     normalized('ток'), # Является частицей, видимо\n",
    "#     normalized('тип'), # Является частицей, видимо\n",
    "# )\n",
    "\n",
    "# ADJF = or_(\n",
    "#     and_(\n",
    "#         gram(\"ADJF\"),\n",
    "#         not_(gram(\"Apro\")), # такой\n",
    "#         not_(dictionary({'съедобное', 'полезные'})),\n",
    "#         # Захватывает Допустим как Допустимый\n",
    "#         not_(gram(\"VERB\")),\n",
    "#     ),\n",
    "#     # орбитальная:ADJF пилотируемая:PRTF станция\n",
    "#     and_(\n",
    "#         gram(\"PRTF\"),\n",
    "#         not_(gram(\"Subx\")),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# PREP = gram(\"PREP\")\n",
    "# CONJ = gram(\"CONJ\")\n",
    "\n",
    "# кузнецкий (и) угольный бассейн\n",
    "# DEPENDENT = rule(\n",
    "#     rule(\n",
    "#         ADJF.interpretation(\n",
    "#             Collocation.dependent\n",
    "#         ),\n",
    "#         CONJ.optional()\n",
    "#     ).optional(),\n",
    "#     ADJF.interpretation(\n",
    "#         Collocation.dependent\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# # ягода малинка\n",
    "# MAIN = rule(\n",
    "#     NOUN.interpretation(\n",
    "#         Collocation.main\n",
    "#     ),\n",
    "#     eq('-').optional(),\n",
    "#     NOUN.optional().interpretation(\n",
    "#         Collocation.main\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# COLLOCATION = or_(\n",
    "#     MAIN,\n",
    "#     # сладкая и сочная ягода малинка поваренная пищевая\n",
    "#     # курица, курицу\n",
    "#     # зубная паста\n",
    "#     rule(DEPENDENT.optional(), MAIN),\n",
    "#     # орехи грецкие\n",
    "#     rule(MAIN, DEPENDENT.optional(),\n",
    "#     ),\n",
    "# ).interpretation(\n",
    "#     Collocation\n",
    "# )\n",
    "\n",
    "# ENTITY = or_(\n",
    "#     COLLOCATION.interpretation(\n",
    "#         Entity.subject\n",
    "#     ),\n",
    "#     # обвиняемый по делу\n",
    "#     # шампунь для волос\n",
    "#     # бутерброды с кока колой\n",
    "#     # мороженое в виде какашки\n",
    "#     # артемовск в донецкой области, партия правого крыла\n",
    "#     # служащий сферы обслуживания\n",
    "#     # диски сложенные стопочкой predicate\n",
    "#     rule(\n",
    "#         COLLOCATION.interpretation(\n",
    "#             Entity.subject\n",
    "#         ),\n",
    "#         rule(\n",
    "#             PREP.optional().interpretation(\n",
    "#                 Entity.prep\n",
    "#             ),\n",
    "#             COLLOCATION.interpretation(\n",
    "#                 Entity.predicate\n",
    "#             )\n",
    "#         ),\n",
    "#     ),\n",
    "# ).interpretation(\n",
    "#     Entity\n",
    "# )\n",
    "\n",
    "\n",
    "def extract_entity(command: str) -> Optional[str]:\n",
    "    matches = list(parser.findall(command))\n",
    "    \n",
    "    if not matches:\n",
    "        return None\n",
    "\n",
    "    match = max(matches, key=lambda m: len(m.tokens))\n",
    "\n",
    "    result = []\n",
    "    return\n",
    "    e = match.fact\n",
    "\n",
    "    print(e)\n",
    "\n",
    "    s = e.subject\n",
    "    sm0 = morph.parse(s.main[0])[0]\n",
    "\n",
    "    dependent_tags = {'nomn', sm0.tag.number}\n",
    "\n",
    "    if sm0.tag.number == 'sing':\n",
    "        dependent_tags.add(sm0.tag.gender)\n",
    "\n",
    "    for dep in s.dependent:\n",
    "        result.append(\n",
    "            morph.parse(dep)[0].inflect(dependent_tags).word\n",
    "        )\n",
    "\n",
    "    if sm0.tag.animacy == 'anim':\n",
    "        result.append(sm0.inflect({'nomn'}).word)\n",
    "    else:\n",
    "        result.append(sm0.word)\n",
    "\n",
    "    result += s.main[1:]\n",
    "\n",
    "\n",
    "    if e.predicate:\n",
    "        if e.prep:\n",
    "            result.append(e.prep)\n",
    "\n",
    "        result += e.predicate.dependent\n",
    "        result += e.predicate.main\n",
    "    \n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "# print(extract_entity(\"раковина беспозвоночного животного\"))\n",
    "\n",
    "\n",
    "for doc in db.eat_eatable.find({'is_hidden': {\"$ne\":True}}):\n",
    "# for doc in db.eat_guess.find({\"is_user\": True}):\n",
    "    command = doc['name']\n",
    "    expected = doc['name']\n",
    "\n",
    "    if '-' in command or '/' in command or '\"' in command or '(' in command:\n",
    "        continue\n",
    "\n",
    "    matches = parser.findall(command)\n",
    "\n",
    "    if not matches:\n",
    "        print(\"❗️\", command)\n",
    "        continue\n",
    "\n",
    "    entities = []\n",
    "    found = False\n",
    "    for match in matches:\n",
    "        entity = command[match.span.start:match.span.stop]\n",
    "        entities.append(entity)\n",
    "\n",
    "        if entity == expected:\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        print(\" | \".join(entities), \"!=\", expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipymarkup import show_span_ascii_markup as show_markup\n",
    "\n",
    "\n",
    "for doc in db.eat_guess.find({\"is_user\": True}):\n",
    "    command = doc['riddle']\n",
    "\n",
    "\n",
    "    match = parser.find(command)\n",
    "\n",
    "    if not match:\n",
    "        print(\"❗️\", command)\n",
    "        continue\n",
    "\n",
    "    show_markup(command, [match.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
